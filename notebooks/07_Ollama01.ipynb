{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d36712",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9658288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564adaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a technical writer, I've had the pleasure of collaborating with linguists (or \"langsmiths\" as you call them) on various projects. When it comes to testing, linguists can bring a unique set of skills to the table.\n",
      "\n",
      "Here are some ways langsmiths can help with testing:\n",
      "\n",
      "1. **Native speaker feedback**: Langsmiths who are native speakers of the target language can provide invaluable feedback on the accuracy and naturalness of your test scenarios, test cases, or user interface text.\n",
      "2. **Cultural insight**: A langsmith's cultural background can inform the development of test scenarios that account for regional differences in usage, idioms, or conventions.\n",
      "3. **Language-specific testing**: Langsmiths are well-versed in the nuances of their native language(s). They can help create test cases that target specific linguistic features, such as grammar, syntax, or formatting.\n",
      "4. **Error detection and reporting**: When a langsmith is familiar with both the source text and the target text, they can identify errors or inconsistencies more effectively than a non-native speaker.\n",
      "5. **Test data creation**: Langsmiths can generate test data that reflects real-world usage patterns, including common phrases, sentences, or dialogues.\n",
      "\n",
      "In summary, by collaborating with a langsmith during testing, you can ensure that your product or service is not only linguistically accurate but also culturally relevant and user-friendly for the target audience.\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3\")\n",
    "output = StrOutputParser()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a skilled technical writer.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \"{user_input}\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm | output\n",
    "response = chain.invoke({\"user_input\": \"how can langsmith help with testing?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e58774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::ROUND 2:::\n",
      "Based on the given context, Langsmith can help with testing in several ways:\n",
      "\n",
      "* Every playground run is logged in the system and can be used to create test cases or compare with other runs.\n",
      "* Langsmith allows developers to collect more data on how their LLM applications are performing in real-world scenarios through beta testing.\n",
      "* It supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria.\n",
      "* Langsmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios.\n",
      "* It provides a playground environment for rapid iteration and experimentation, allowing you to quickly test out different prompts and models.\n",
      "\n",
      "These features enable developers to create test cases, compare different versions of their applications, and track regressions/improvements, making it easier to refine and improve the performance of their LLM applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Invoke chain with RAG context\n",
    "llm = Ollama(model=\"llama3\")\n",
    "## Load page content\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "\n",
    "## Vector store things\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "vector_store = FAISS.from_documents(split_documents, embeddings)\n",
    "\n",
    "## Prompt construction\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "            Answer the following question only based on the given context\n",
    "                                                    \n",
    "            <context>\n",
    "            {context}\n",
    "            </context>\n",
    "                                                    \n",
    "            Question: {input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "## Retrieve context from vector store\n",
    "docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, docs_chain)\n",
    "\n",
    "## Winner winner chicken dinner\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(\":::ROUND 2:::\")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e5dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(\"faiss_index\") # Replace faiss_index with name of your choice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
