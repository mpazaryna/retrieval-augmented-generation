{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7c4c87",
   "metadata": {},
   "source": [
    "# Router Engine\n",
    "\n",
    "Given a query, the router will pick what to run.  This shows dynamic query understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c97c9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a27e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv_path = dotenv.find_dotenv()\n",
    "dotenv.load_dotenv(dotenv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc9b4f4-64d4-4266-9889-54db90e00ee9",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is required since Juypyter notebook is not async by default.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fca250",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7f012d-dcd3-4881-a568-72dd27d79159",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "\n",
    "lightning_ai=\"/teamspace/studios/this_studio/retrieval-augmented-generation/data/transcript.pdf\"\n",
    "github_codespace=\"./data/transcript.pdf\"\n",
    "documents = SimpleDirectoryReader(input_files=[lightning_ai]).load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48a301",
   "metadata": {},
   "source": [
    "## Split document into nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a537bc0-78ee-4dda-a43f-60fd80062df6",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61afc8bf",
   "metadata": {},
   "source": [
    "## Define LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de0660ee-b231-4351-b158-d8ad023e00b5",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c7559",
   "metadata": {},
   "source": [
    "## Define Indexes\n",
    "\n",
    "- Summary Index \n",
    "- Vector Index \n",
    "\n",
    "Think of the index as a set of metadata over the data.  Different indexes have different index behavior.  A vector index indexes node through text embeddings.  Querying a vector index in llamaindex will return the most similar nodes. A summary index is also a simple index, and a query to it will return all of the nodes that are currently in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d01b01-bc74-432a-8d92-07b9e86498b0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9898d3f",
   "metadata": {},
   "source": [
    "## Turn the indexes into query engines\n",
    "\n",
    "Each query engine represents an interface over the data that has been stored in the index.  It combines retrieval with LLM synthesis.  Each engine is specific for a certain kind of question.  This is where the use case of the router becomes significant.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44cd7046-c714-4920-b077-b3ded917862f",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6d44d",
   "metadata": {},
   "source": [
    "## Create query tools\n",
    "\n",
    "The query tools are simply the query engine with additional metadata.  The metadata reflects the kind of information the tool can answer.  For example, asking it to \"summarize\" the article would launch the summary_tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a1d6d75-247e-426a-8ef4-b49225c24796",
   "metadata": {
    "height": 285,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to the document.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the document.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2c152",
   "metadata": {},
   "source": [
    "## Define Router\n",
    "\n",
    "Llamaindex provides several diffent selectors.  \n",
    "\n",
    "- The LLM selectors use the LLM to output a JSON that is parsed, and corresponding indexes are queried.\n",
    "- The Pydantic selects use the OpenAI Function Calling API to produce pydantic selection objects rather than raw JSON.\n",
    "\n",
    "In this example we are using the LLMSingleSelector. The RouterQueryEngine takes in a selector type as well as a set of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00734d7c-638a-4d63-ab1f-7f5a92a65119",
   "metadata": {
    "height": 217,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    # allows us to view the intermediate steps of the query engine\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe3f0a76-68a8-444d-867f-d084bb3ff112",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Useful for summarization questions related to the document..\n",
      "\u001b[0mThe document discusses the importance of bridging the Product Company Gap, emphasizing the need to move beyond just product Market fit. It highlights the significance of building a product that has a good value proposition, targets a minimum viable segment, and focuses on repeatable success. The discussion also covers aspects like designing for product go-to-market fit, creating instant and ongoing value for customers, and playing well in the ecosystem through partnerships. Additionally, it touches on pricing strategies, simplifying product installation, and leveraging partnerships to accelerate business growth. Overall, the document provides insights on how to develop a product that can scale into a successful company.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3fedea0-f2a9-46bb-8aaf-287df65b8fff",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8c31b3-8e22-4ad9-9825-b8de21bd03c0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: Useful for retrieving specific context from the document..\n",
      "\u001b[0mTo build a product according to this document, you should start by designing a product for go-to-market fit, ensuring that you create a minimum viable product (MVP) that addresses a valuable problem. It is crucial to triple check the value proposition before investing in hiring engineers or spending money to ensure that the product solves a problem that customers are willing to pay for. Additionally, it is important to design the product with scalability in mind, considering factors like go-to-market strategy, pricing, and building a business model that supports scaling the product into a successful company.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do you build a product according to this document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed060ee",
   "metadata": {},
   "source": [
    "## Let's put everything together\n",
    "\n",
    "Code from the above cells, have been put into the utils helper method.  Builds the query engine with both vector and summary search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8f92e0b-1c54-489b-b8dd-41ebaafb380a",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from utils import get_router_query_engine\n",
    "\n",
    "query_engine = get_router_query_engine(\"./data/transcript.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec1a43f3-77dc-472a-8adc-56551c00a0ff",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The choice is more relevant as it specifically mentions retrieving specific context from the MetaGPT paper..\n",
      "\u001b[0mThe document appears to be a transcript from a discussion or presentation focused on product development and design principles. It includes conversations about simplicity in product design, the importance of focusing on solving basic problems easily, and the value of creating products that address specific needs. The discussion also touches on the concept of minimum viable segments, the importance of proving value early on, and strategies for streamlining time to value for different types of companies.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the document.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37d8cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The paper provides specific context from the MetaGPT paper, which may include coding examples..\n",
      "\u001b[0mThe paper does not provide specific coding examples.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Does the paper provide specific coding example?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
